# ML_MAI

# Лабораторные работы по машинному обучению

## Задачи
1. Классификация тональности отзывов о ресторанах
2. Регрессия — предсказание цены аренды жилья в Нью-Йорке (Airbnb)

## Исследованные алгоритмы

| № лаб | Алгоритм                     | Библиотека                     | Своя реализация |
|-------|------------------------------|--------------------------------|-----------------|
| 1     | K-Nearest Neighbors          | Да                             | Да              |
| 2     | Линейная / Логистическая регрессия | Да                       | Да              |
| 3     | Решающее дерево              | Да                             | Да              |
| 4     | Случайный лес (Random Forest)| Да                             | Да              |
| 5     | Градиентный бустинг          | XGBoost                        | Да              |

## Результаты (лучшие модели на тестовых данных)

### Классификация отзывов
| Модель                  | Accuracy | F1-score |
|-------------------------|----------|----------|
| XGBoost                 | 0.9020   | 0.9008   |
| Random Forest           | 0.8620   | 0.8601   |
| Decision Tree           | 0.8077   | 0.8054   |
| Logistic Regression     | 0.8000   | 0.7970   |
| KNN                     | 0.7600   | 0.7590   |

### Регрессия цен на жильё (MAE)
| Модель                  | MAE     | RMSE    |
|-------------------------|---------|---------|
| XGBoost                 | 43.87   | 68.21   |
| Random Forest           | 49.12   | 76.45   |
| Decision Tree           | 56.13   | 91.83   |
| Linear Regression       | 60.07   | 95.87   |
| KNN                     | 62.20   | 98.50   |

## Выводы

1. **Градиентный бустинг (XGBoost)** показал наилучшее качество на обеих задачах.
2. Случайный лес второе место.
3. Одиночные модели (дерево, линейная регрессия, KNN) значительно уступают ансамблям.
4. Использование признаков-инженерии (`log_price`, расстояние до центра, n-граммы) существенно улучшило результат.

Градиентный бустинг — наиболее эффективный метод среди изученных.
